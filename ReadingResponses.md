## Reading Prompt

For each reading, pick two sentence/passages. One should be where the had an AHA! moment and one that is unclear, confusing, or needs further explanation. Provide a sentence or two explaining each of your selections.  Provide your response below.

 

### 2017-09-18: Distant reading
My moment of confusion came with Schulz's "What Is Distant Reading?" when she explains that although Moretti would like to treat literary analysis like a science, it doesn't work out perfectly because literature has been created by humans (and not by nature), so it does not abide by a consistent set of laws. How can we quantify data in literature if what "he perceives inside stories are as imposed as exposed"? Moretti even states in his work *Graphs, Maps, Trees* that "quantitative research provides a type of data which is ideally independent of interpretations," which Schulz argues is simply not true.

My AHA moment came when Moretti admitted that the history of literature (in this case the relationship between genres and generations) is not exact enough to make perfect predictions or analyses, conceding that "'generation' is itself a very questionable concept." Though this didn't necessarily provide more clarity, it did provide a necessary critique of the practice of distant reading and an explanation of the origin of the defining characteristics of generations.

### 2017-09-25: Topic modeling I
The part of Megan Brett's article that confused me the most is the size of the corpus needed for topic modeling. Why does MALLET need a minimum of 1000 works? What if I don't need such a large sample size?
However, her analogy of manual text-analysis with a highlighter helped me to understand the MALLET program as a way of identifying key concepts, but in a much more efficient manor than one person color-coding an article with highlighters.

The most confusing part of Wiengart and Meeks' article was their reference to "gray literature". I believe they are referring to nontraditional forms of literature like twitter threads and blog posts, but even then, how do those forms of "literature" apply to topic modeling?
My AHA moment came towards the end of the article when they admitted that topic modeling is "not an upgrade from simplistic human-driven research, but merely {another tool} in the ever-growing shed." This helped to solidify both the practicality and potential of these kinds of tools. 

With Underwood and Goldstone's discussion of the benefits and drawbacks of topic modeling, I began to wonder how one can clearly and efficiently map these groups of randomly associated words into topics useful for analysis. How can one tell what is a relevant connection and what is completely random without close reading such a large body of texts.
One of the more clear points in this article was the analogy of the puzzle. According to Underwood and Goldstone, the differences among topic modeling programs (or even every time you run the same program) are like the differences between a 100 piece puzzle and a 150 piece puzzle. Each individual piece will look different, but once each puzzle is completed, they form the same image.

Though Martha Ballard's diary presented a more concrete example of topic modeling, I am still confused as to how these groups of words can be analyzed effectively. With such a large body of work, it astonishes me that Blevins was able to glean something out of these topics, without actually reading every diary entry. 
I suppose Belvin's visuals were most helpful to me. By mapping these correlating groups of words (that I still don't quite understand), he was able to clearly present trends of each of these topics, making his point much more clear by properly labelling his graphs and providing adequate context. 

### 2017-10-02: Topic Modeling II
The part of the text analysis of Trump's tweets that was most confusing to me was the coding. I'm still confused as to how these programs work, and although this article provided a little more context than others, I'm still lost in all of the coding language and usage of each different program.
My AHA moment was the clarity of the article, which included famililar examples and clear graphs with adequate explanations. Though I didn't understand everything that the article explained, the graphs made the author's argument much more clear by providing direct evidence for his claims. 

My main point of confusion when reading through the description of topic modeling in Mining the Dispatch was how they decided the categories for topics. What if there were more topics within those thousands of newspapers? How can they be sure that every article is in the correct topic?
My AHA moment came with the graphs again. The comparisons between predicted fugitive slave ads (based on topics) and close-read fugitive slave acts was astonishing. I was surprised at how accurate the program was, though I appreciated the concession the author made at the end: though the program isn't always correct, sometimes there are topics so out of the blue that we can truly discover something new in such a familiar history. 

I am very curious as to how the Signs at 40 project was able to link general topics back to their specific articles with such a large corpus. I was also generally confused by the organization of the project; it was hard to tell what I was looking for. On the flip side, my AHA moment included the number of different, yet related topics the project included. Though I was confused as to what topic meant what, I believe the project did a decent job of linking the different topics together in a linear fashion. The visual on the homepage also helped me to understand the topics a little better.

My biggest question in regards to the article about Harper Lee would be how scholars initially made the assumption that *To Kill a Mockingbird* was ghostwritten by Truman Capote. With so much evidence for Lee's authentic authorship (though with the added concession of a clear presence of editing), I'm curious as to what the arguments from the other side are. On an unrealted note, my AHA moment came at the mention of Rybicki's research into the voice of translators in scholarly work. I've always struggled with some of the grammar or word choice in translated works, and I've always wondered how much of that was in the original writing, and how much came from the translator. I'm very curious as to how tools like MALLET can be used to find the voice of an author.

### 2017-10-09: Stylometry

### 2017-10-18: Spatial history
My AHA moment in Bodenhamer's explanation of the link between GIS technologies and history was the revolutionary change in how we view time and space. I had never considered how humanity viewed time before the instantaneity of telephones, television, or the internet. On that note, I am still unsure of why this link is important. Perhaps it's because I'm used to that instantaneity, but this article seemed like it was explaining something we already practiced (such as spatial mapping or the weblike nature of history), whether or not we could articulate it ourselves. 

My WTF moment in White's *What is Spatial History?* and his description of the concept of space around line 7. Before reading this article, I thought I had a pretty good idea of space as geographical or physical space, but this explanation just complicated that concept further.
His separation of space into representational space, absolute space, and relational space, helped to clarify things into an AHA moment, but I'm still confused as to how each of these forms of space are mapped and coordinated with each other.

Shnayder's explanation of the *Shaping the Wild West Geodatabase* was my moment of clarification for the past few articles. This example helped to clarify some of the more vague concepts surrounding space in the digital humanities. 
Towards the end of the article, Shnayder explained the varying levels of access to the project and its management. This made me wonder why and how different editors could change the content so easily when the technology seems fairly straightforward. 

My AHA moment with the American Panorama project came with the explanation of some of the maps, which was incredibly helpful in regards to the sheer amount of data presented in each map. However, even with the explanations, some layouts were more clear than others. The Executive Abroad page was particularly confusing for me, even with the explanation, because each location was so small that it was very difficult to choose a small city. Additionally, though I liked the visual of the circle of presidents surrounding the globe, it made the graph of the relative number of visits difficult to read. The color key at the bottom also described a number of visits, though the colored dots actually corresponded to cities on the different continents, which was another point of confusion. 

The most interesting part of *Redlining in Philadelphia* to me was the fact that it challenged previous research on redlining, such as Richmond's own project. It was helpful to see another side to the argument I've been talking about in multiple classes this semester. However, I'm confused as to the differences in method that led to this different conclusion. What are the differences in method, and how did they have this much of an impact on the formation of the history of redlining? 



### 2017-10-23: Mapping: A Critical Introductin

### 2017-11-06: Social networks

### 2017-11-13: Layering Networks

### 2017-11-27: Critical DH
